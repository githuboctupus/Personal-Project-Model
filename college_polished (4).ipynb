{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1VYpfR7sQcj"
      },
      "outputs": [],
      "source": [
        "!curl -L -o rescuenet.zip \"https://www.dropbox.com/scl/fo/ntgeyhxe2mzd2wuh7he7x/AFIchlfjVO_7MzPcNc1ZOHE/RescueNet?rlkey=6vxiaqve9gp6vzvzh3t5mz0vv&subfolder_nav_tracking=1&st=cpmz72mg&dl=1\"  #download the main rescuenet image archive directly into the notebook workspace\n",
        "!curl -L -o colormask.zip \"https://www.dropbox.com/scl/fo/ntgeyhxe2mzd2wuh7he7x/AK7z2KSL2Df2igYzGrrHlYs/ColorMasks-RescueNet?dl=0&rlkey=6vxiaqve9gp6vzvzh3t5mz0vv&subfolder_nav_tracking=1&d=1\"  #download the color-mask archive that pairs with the images\n",
        "!unzip rescuenet.zip -d rescuenettrain/  #unzip images into a dedicated folder so paths are predictable\n",
        "!unzip colormask.zip -d rescuenetmask/   #unzip masks into a parallel folder keeping train/test structure\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#core libs\n",
        "#these are the standard python libraries and cv tools i use for file io and image handling\n",
        "import os, glob, math, random  #filesystem ops, pattern search, math helpers, rng\n",
        "import numpy as np             #array math\n",
        "import cv2                     #opencv for image resizing and padding\n",
        "from PIL import Image          #reliable png/jpg reading\n",
        "\n",
        "#torch & utils\n",
        "#pytorch core plus dataloaders and a progress bar for visibility\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "#external libs used later\n",
        "#albumentations handles all of my image/mask augmentations\n",
        "import albumentations as A\n",
        "!pip install segmentation_models_pytorch  #install segmentation models so i can use deeplabv3+ quickly\n",
        "import segmentation_models_pytorch as smp\n",
        "from google.colab import drive  #used to persist checkpoints to drive\n",
        "\n",
        "#pytorch perf/precision knobs\n",
        "#enable safe loading, allow tf32 paths, and prefer higher matmul precision when available\n",
        "torch.serialization.add_safe_globals([__import__(\"numpy\")._core.multiarray._reconstruct])  #handle numpy arrays in torch.load safety list\n",
        "torch.backends.cuda.matmul.allow_tf32 = True  #use tf32 on ampere+ for speed without big accuracy loss\n",
        "torch.backends.cudnn.allow_tf32 = True        #same idea for cudnn convolutions\n",
        "try:\n",
        "    torch.set_float32_matmul_precision(\"high\")  #pytorch 2.x matmul tuning; ok to skip if not supported\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "#shared constants used throughout\n",
        "IGNORE_INDEX = 255  #mask value to ignore when computing loss/metrics\n"
      ],
      "metadata": {
        "id": "TCHwadwbsZqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fast color(rgb)->class-id lut\n",
        "#i convert 3-channel color masks into integer class ids using a 256^3 lookup table\n",
        "def build_color_lut(color_map, ignore_index=IGNORE_INDEX):\n",
        "    lut = np.full((256,256,256), ignore_index, dtype=np.uint8)  #pre-fill with ignore so unknown colors are skipped\n",
        "    for (r,g,b), cid in color_map.items():\n",
        "        lut[r,g,b] = cid\n",
        "    return lut\n",
        "\n",
        "def rgbmask_to_ids(mask_rgb: np.ndarray, lut: np.ndarray) -> np.ndarray:\n",
        "    flat = mask_rgb.reshape(-1,3)                                #flatten to n×3 so we can index the lut\n",
        "    ids = lut[flat[:,0], flat[:,1], flat[:,2]]                   #vectorized mapping from color to class id\n",
        "    return ids.reshape(mask_rgb.shape[:2])                       #restore to h×w\n",
        "\n",
        "class RescueNetSegDataset(Dataset):\n",
        "    \"\"\"\n",
        "    image_dir: JPG/PNG images (e.g., .../train-org-img)\n",
        "    mask_dir:  color masks named like <stem>_lab.png\n",
        "    augment:   Albumentations Compose (image+mask) WITHOUT Normalize (recommended)\n",
        "    normalize: Albumentations A.Normalize(mean,std) OR None\n",
        "    \"\"\"\n",
        "    #this dataset pairs each image with its color mask, applies augs, converts to tensors\n",
        "    def __init__(self, image_dir, mask_dir, augment=None, normalize=None,\n",
        "                 color_map=None, return_paths=False):\n",
        "        assert color_map is not None, \"Provide COLOR_MAP\"  #i require a color map for lut conversion\n",
        "        #allow jpg/jpeg/png\n",
        "        exts = (\"*.jpg\",\"*.jpeg\",\"*.png\")\n",
        "        self.image_paths = sorted(sum([glob.glob(os.path.join(image_dir, e)) for e in exts], []))\n",
        "\n",
        "        #map masks by stem (strip '_lab')\n",
        "        mask_files = glob.glob(os.path.join(mask_dir, \"*.png\"))\n",
        "        mask_dict = {os.path.basename(m).replace(\"_lab.png\",\"\"): m for m in mask_files}\n",
        "\n",
        "        self.mask_paths = []\n",
        "        for ip in self.image_paths:\n",
        "            stem = os.path.splitext(os.path.basename(ip))[0]\n",
        "            mp = mask_dict.get(stem)\n",
        "            if mp is None:\n",
        "                raise RuntimeError(f\"No mask found for image: {ip}\")  #hard fail so data issues are caught early\n",
        "            self.mask_paths.append(mp)\n",
        "\n",
        "        self.augment = augment\n",
        "        self.normalize = normalize\n",
        "        self.return_paths = return_paths\n",
        "        self.lut = build_color_lut(color_map, IGNORE_INDEX)  #precompute lut once per dataset\n",
        "\n",
        "    def __len__(self): return len(self.image_paths)  #standard dataset length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ip, mp = self.image_paths[idx], self.mask_paths[idx]\n",
        "\n",
        "        img = np.array(Image.open(ip).convert(\"RGB\"))        #HWC uint8\n",
        "        msk_rgb = np.array(Image.open(mp).convert(\"RGB\"))    #HWC uint8\n",
        "\n",
        "        #ensure same size before aug\n",
        "        if msk_rgb.shape[:2] != img.shape[:2]:\n",
        "            msk_rgb = cv2.resize(msk_rgb, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)  #preserve labels\n",
        "\n",
        "        ids = rgbmask_to_ids(msk_rgb, self.lut)              #HxW uint8/255\n",
        "        ids = ids.astype(np.int64, copy=False)\n",
        "\n",
        "        if self.augment is not None:\n",
        "            out = self.augment(image=img, mask=ids)          #apply joint augs\n",
        "            img, ids = out[\"image\"], out[\"mask\"]\n",
        "            ids = ids.astype(np.int64, copy=False)\n",
        "\n",
        "        #--- normalization guard ---\n",
        "        if self.normalize is not None:\n",
        "            #use only this Normalize (float32, mean/std)\n",
        "            img = self.normalize(image=img)[\"image\"]\n",
        "        else:\n",
        "            #if already float32 (e.g., some augs), don't rescale again\n",
        "            if img.dtype == np.uint8:\n",
        "                img = img.astype(np.float32) / 255.0\n",
        "            elif img.dtype != np.float32:\n",
        "                img = img.astype(np.float32) / 255.0  #fallback\n",
        "\n",
        "        x = torch.from_numpy(img.transpose(2,0,1))  #C,H,W float32\n",
        "        y = torch.from_numpy(ids)                   #H,W int64\n",
        "\n",
        "        if self.return_paths:\n",
        "            return x, y, (ip, mp)\n",
        "        return x, y\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"RescueNetSegDataset(n={len(self)}, img0='{os.path.basename(self.image_paths[0])}')\"  #quick sanity view\n"
      ],
      "metadata": {
        "id": "oT35HYkyuDPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q albumentations segmentation_models_pytorch  #ensure correct versions are present for colab runtime\n",
        "\n",
        "#this custom pad transform enforces a minimum canvas size with constant borders for both image and mask\n",
        "class PadIfNeededConst(A.DualTransform):\n",
        "    \"\"\"\n",
        "    Pad to (min_height, min_width) using CONSTANT border.\n",
        "    position: 'bottom_right' (default), 'center', or 'top_left'\n",
        "    - image padding uses `value` (tuple or int)\n",
        "    - mask padding uses `mask_value` (int), e.g., IGNORE_INDEX\n",
        "    \"\"\"\n",
        "    def __init__(self, min_height, min_width,\n",
        "                 border_mode=cv2.BORDER_CONSTANT,\n",
        "                 value=0, mask_value=255, position='bottom_right',\n",
        "                 always_apply=False, p=1.0):\n",
        "        super().__init__(always_apply, p)\n",
        "        self.min_height = int(min_height)\n",
        "        self.min_width  = int(min_width)\n",
        "        self.border_mode = border_mode\n",
        "        self.value = value\n",
        "        self.mask_value = int(mask_value)\n",
        "        self.position = position\n",
        "        if self.border_mode != cv2.BORDER_CONSTANT:\n",
        "            raise NotImplementedError(\"PadIfNeededConst currently supports only BORDER_CONSTANT.\")  #keep scope simple\n",
        "\n",
        "    def _pad_amounts(self, h, w):\n",
        "        dh = max(0, self.min_height - h)\n",
        "        dw = max(0, self.min_width  - w)\n",
        "        if dh == 0 and dw == 0:\n",
        "            return 0, 0, 0, 0\n",
        "        if self.position == 'center':\n",
        "            top    = dh // 2\n",
        "            bottom = dh - top\n",
        "            left   = dw // 2\n",
        "            right  = dw - left\n",
        "        elif self.position == 'top_left':\n",
        "            top, left = 0, 0\n",
        "            bottom, right = dh, dw\n",
        "        else:  # 'bottom_right' (anchor top-left)\n",
        "            top, left = 0, 0\n",
        "            bottom, right = dh, dw\n",
        "        return top, bottom, left, right\n",
        "\n",
        "    def apply(self, img, **params):\n",
        "        h, w = img.shape[:2]\n",
        "        top, bottom, left, right = self._pad_amounts(h, w)\n",
        "        if top == bottom == left == right == 0:\n",
        "            return img\n",
        "        #handle scalar vs 3-tuple image value\n",
        "        val = self.value\n",
        "        if img.ndim == 2 and isinstance(val, (tuple, list)):\n",
        "            val = val[0] if len(val) else 0\n",
        "        return cv2.copyMakeBorder(img, top, bottom, left, right,\n",
        "                                  self.border_mode, value=val)\n",
        "\n",
        "    def apply_to_mask(self, mask, **params):\n",
        "        h, w = mask.shape[:2]\n",
        "        top, bottom, left, right = self._pad_amounts(h, w)\n",
        "        if top == bottom == left == right == 0:\n",
        "            return mask\n",
        "        return cv2.copyMakeBorder(mask, top, bottom, left, right,\n",
        "                                  self.border_mode, value=self.mask_value)\n",
        "\n",
        "    def get_transform_init_args_names(self):\n",
        "        return (\"min_height\", \"min_width\", \"border_mode\", \"value\", \"mask_value\", \"position\")\n"
      ],
      "metadata": {
        "id": "8JGRM6RBuQ68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==== device & perf flags ====\n",
        "#select gpu if available and enable cudnn autotune; tf32 flags were set earlier already\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True  #let cudnn pick fastest conv algorithms\n",
        "\n",
        "#amp: prefer bf16 on newer nvidia; else fall back to fp16 with grad scaler\n",
        "USE_BF16 = True\n",
        "AMP_DTYPE = torch.bfloat16 if (USE_BF16 and torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else None\n",
        "SCALER_ENABLED = (device.type == \"cuda\" and AMP_DTYPE is None)  #scaler is only needed for fp16\n",
        "\n",
        "#====hyperparams ====\n",
        "#these are the core knobs for training; i tuned around gpu limits and validation stability\n",
        "BATCH        = 16\n",
        "ACCUM_STEPS  = 1\n",
        "EPOCHS       = 40\n",
        "LR           = 1e-4\n",
        "WD           = 1e-4\n",
        "GRAD_CLIP_N  = 1.0\n",
        "\n",
        "#colab drive (kept)\n",
        "#mount google drive so i can save best checkpoints across sessions\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "BASE = \"/content/drive/MyDrive/college_data\"\n",
        "os.makedirs(BASE, exist_ok=True)\n",
        "\n",
        "#==== project globals assumed present elsewhere ====\n",
        "#color map for mask decoding and essential paths for images/masks\n",
        "#COLOR_MAP, IGNORE_INDEX, RescueNetSegDataset, PadIfNeededConst, build_color_lut, rgbmask_to_ids\n",
        "ROOT_MASK = \"rescuenetmask\"\n",
        "ROOT_IMG  = \"rescuenettrain\"\n",
        "\n",
        "COLOR_MAP = {\n",
        "    (0, 0, 0): 0,  # background\n",
        "    (61, 230, 250): 1,  # water\n",
        "    (180, 120, 120): 2, # building-no-damage\n",
        "    (235, 255, 7): 3,   # building-medium-damage\n",
        "    (255, 184, 6): 4,   # building-major-damage\n",
        "    (255, 0, 0): 5,     # building-total-destruction\n",
        "    (255, 0, 245): 6,   # vehicle\n",
        "    (140, 140, 140): 7, # road-clear\n",
        "    (160, 150, 20): 8,  # road-blocked\n",
        "    (4, 250, 7): 9,     # tree\n",
        "    (0, 101, 140): 10,  # pool\n",
        "}\n",
        "NUM_CLASSES = max(COLOR_MAP.values()) + 1\n",
        "LUT = build_color_lut(COLOR_MAP, IGNORE_INDEX)  #prebuild lut once\n",
        "\n",
        "#==== transforms ====\n",
        "#pull imagenet preprocessing stats for the chosen encoder\n",
        "enc = \"resnet50\"\n",
        "pp = smp.encoders.get_preprocessing_params(enc, pretrained=\"imagenet\")\n",
        "mean, std = pp[\"mean\"], pp[\"std\"]\n",
        "\n",
        "#version-robust ShiftScaleRotate (remove unsupported args like value/mask_value)\n",
        "#this gives spatial variety while keeping labels valid via constant borders\n",
        "aug_scale = A.ShiftScaleRotate(\n",
        "    shift_limit=0.02, scale_limit=0.5, rotate_limit=10,\n",
        "    border_mode=cv2.BORDER_CONSTANT, p=0.35\n",
        ")\n",
        "\n",
        "#version-robust CoarseDropout: prefer new api, fall back to old\n",
        "#this simulates occlusions/missing pixels and encourages robustness\n",
        "try:\n",
        "    coarse = A.CoarseDropout(\n",
        "        holes_range=(1, 2),\n",
        "        max_height=48, max_width=48,\n",
        "        fill_value=0, mask_fill_value=IGNORE_INDEX, p=0.15\n",
        "    )\n",
        "except TypeError:\n",
        "    coarse = A.CoarseDropout(\n",
        "        min_holes=1, max_holes=2,\n",
        "        min_height=24, max_height=48,\n",
        "        min_width=24,  max_width=48,\n",
        "        fill_value=0, mask_fill_value=IGNORE_INDEX, p=0.15\n",
        "    )\n",
        "\n",
        "def _contig(x, **kwargs): return np.ascontiguousarray(x)  #avoid copies later in torch.from_numpy\n",
        "make_contig = A.Lambda(image=_contig, mask=_contig)\n",
        "\n",
        "CROP = 768   #if you try 832, reduce batch to fit vram\n",
        "\n",
        "#final train pipeline: resize→pad→crop with objects→color/motion augs→normalize\n",
        "train_tfms = A.Compose([\n",
        "    A.LongestMaxSize(max_size=1024),\n",
        "    A.HorizontalFlip(p=0.30),\n",
        "    A.VerticalFlip(p=0.30),\n",
        "    A.RandomRotate90(p=0.40),\n",
        "    aug_scale,\n",
        "    A.SmallestMaxSize(max_size=CROP),\n",
        "    PadIfNeededConst(min_height=1024, min_width=1024,\n",
        "                     border_mode=cv2.BORDER_CONSTANT, value=(0,0,0), mask_value=IGNORE_INDEX),\n",
        "    A.CropNonEmptyMaskIfExists(CROP, CROP, ignore_values=[IGNORE_INDEX], p=1.0),\n",
        "    A.ColorJitter(brightness=0.12, contrast=0.12, saturation=0.12, hue=0.02, p=0.33),\n",
        "    A.MotionBlur(blur_limit=(3,7), p=0.10),\n",
        "    coarse,\n",
        "    make_contig,\n",
        "    A.Normalize(mean=mean, std=std, max_pixel_value=255.0),\n",
        "])\n",
        "\n",
        "#validation pipeline: center crop with padding and same normalization\n",
        "val_tfms = A.Compose([\n",
        "    A.SmallestMaxSize(max_size=CROP),\n",
        "    PadIfNeededConst(CROP, CROP, value=(0,0,0), mask_value=IGNORE_INDEX, position='center'),\n",
        "    A.CenterCrop(CROP, CROP),\n",
        "    make_contig,\n",
        "    A.Normalize(mean=mean, std=std, max_pixel_value=255.0),\n",
        "])\n",
        "\n",
        "print(\"created transforms\")  #quick sanity check\n"
      ],
      "metadata": {
        "id": "RTUmXKQwuSUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==== datasets ====\n",
        "#build train/val datasets with shared color map and transform pipelines\n",
        "train_ds_full = RescueNetSegDataset(\n",
        "    image_dir=f\"{ROOT_IMG}/train/train-org-img\",\n",
        "    mask_dir=f\"{ROOT_MASK}/ColorMasks-TrainSet\",\n",
        "    color_map=COLOR_MAP,\n",
        "    augment=train_tfms,\n",
        "    normalize=None,\n",
        ")\n",
        "val_ds_full = RescueNetSegDataset(\n",
        "    image_dir=f\"{ROOT_IMG}/test/test-org-img\",\n",
        "    mask_dir=f\"{ROOT_MASK}/ColorMasks-TestSet\",\n",
        "    color_map=COLOR_MAP,\n",
        "    augment=val_tfms,\n",
        "    normalize=None,\n",
        ")\n",
        "train_ds = train_ds_full\n",
        "val_ds   = val_ds_full\n",
        "print(\"loaded datasets\")\n",
        "print(\"train size:\", len(train_ds))\n",
        "\n",
        "#==== CE class weights from global pixel counts ====\n",
        "#compute per-class weights so cross-entropy doesn’t ignore rare classes\n",
        "def mask_paths_for(subset_or_ds):\n",
        "    if isinstance(subset_or_ds, Subset):\n",
        "        base, idxs = subset_or_ds.dataset, subset_or_ds.indices\n",
        "        return [base.mask_paths[i] for i in idxs]\n",
        "    return list(subset_or_ds.mask_paths)\n",
        "\n",
        "train_mask_paths = mask_paths_for(train_ds)\n",
        "\n",
        "try:\n",
        "    counts = np.load(\"train_counts.npy\")  #reuse cached counts if present\n",
        "except:\n",
        "    counts = np.zeros(NUM_CLASSES, dtype=np.int64)\n",
        "    for mp in tqdm(train_mask_paths, desc=\"Counting pixels (full train)\"):\n",
        "        m = np.array(Image.open(mp).convert(\"RGB\"))\n",
        "        ids = rgbmask_to_ids(m, LUT)\n",
        "        ids = ids[ids != IGNORE_INDEX]\n",
        "        if ids.size:\n",
        "            counts += np.bincount(ids.ravel(), minlength=NUM_CLASSES)\n",
        "    np.save(\"train_counts.npy\", counts)\n",
        "\n",
        "w = np.where(counts > 0, 1.0 / np.sqrt(counts.astype(np.float64)), 0.0)  #inverse sqrt freq\n",
        "nz = w[w > 0]\n",
        "if nz.size: w /= np.median(nz)  #normalize by median to keep scales reasonable\n",
        "print(\"CE weights before clip:\", w.tolist())\n",
        "w = np.clip(w, 0.25, 2.5)  #cap extremes so training remains stable\n",
        "print(\"CE weights after clip:\", w.tolist())\n",
        "class_weights = torch.tensor(w, dtype=torch.float32, device=device)  #move to device for loss\n"
      ],
      "metadata": {
        "id": "5nPnhq6Yuapx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==== rarity-aware image sampler ====\n",
        "#this sampler increases chance of picking images that contain rare classes and rare areas\n",
        "def compute_image_weights(\n",
        "    dataset, lut, num_classes, ignore_index=IGNORE_INDEX,\n",
        "    gamma=1.1, lambda_mix=0.9, cap_min=0.25, cap_max=6.0, show_progress=True\n",
        "):\n",
        "    img_classes = []\n",
        "    class_img_freq = np.zeros(num_classes, dtype=np.int64)\n",
        "    per_img_hist = []\n",
        "    global_pixel_counts = np.zeros(num_classes, dtype=np.int64)\n",
        "\n",
        "    it = dataset.mask_paths\n",
        "    if show_progress: it = tqdm(it, desc=\"Scanning masks (histograms + presence)\")\n",
        "    for mp in it:\n",
        "        m = np.array(Image.open(mp).convert(\"RGB\"))\n",
        "        ids = rgbmask_to_ids(m, lut)\n",
        "        valid = (ids != ignore_index)\n",
        "        ids_v = ids[valid]\n",
        "\n",
        "        hist = np.bincount(ids_v.ravel(), minlength=num_classes).astype(np.int64)\n",
        "        per_img_hist.append(hist)\n",
        "        global_pixel_counts += hist\n",
        "\n",
        "        present = np.flatnonzero(hist > 0)\n",
        "        img_classes.append(present)\n",
        "        for c in present: class_img_freq[c] += 1\n",
        "\n",
        "    #presence term\n",
        "    class_img_freq = np.maximum(class_img_freq, 1)\n",
        "    denom_presence = np.sqrt(class_img_freq.astype(np.float64))\n",
        "    presence_weights = []\n",
        "    for present in img_classes:\n",
        "        presence_weights.append(1.0 if len(present)==0 else float(np.sum(1.0 / denom_presence[present])))\n",
        "    presence_weights = np.asarray(presence_weights, dtype=np.float64)\n",
        "\n",
        "    #rarity term\n",
        "    g = global_pixel_counts.astype(np.float64).copy()\n",
        "    g[g < 1] = 1.0\n",
        "    rarity = 1.0 / (g ** gamma)\n",
        "    rarity_weights = []\n",
        "    for hist in per_img_hist:\n",
        "        total = float(hist.sum())\n",
        "        if total <= 0.0:\n",
        "            rarity_weights.append(1.0)\n",
        "        else:\n",
        "            mix = hist.astype(np.float64) / total\n",
        "            rarity_weights.append(float((rarity * mix).sum()))\n",
        "    rarity_weights = np.asarray(rarity_weights, dtype=np.float64)\n",
        "\n",
        "    #blend + clamp + normalize\n",
        "    w = (1.0 - lambda_mix) * presence_weights + lambda_mix * rarity_weights\n",
        "    w = np.clip(w, cap_min, cap_max)\n",
        "    w /= w.sum()\n",
        "    return torch.as_tensor(w, dtype=torch.double), global_pixel_counts\n",
        "\n",
        "try:\n",
        "    img_weights, counts_recalc = torch.load(\"img_weights_sampler_2.pt\", weights_only=False)  #reuse if cached\n",
        "except:\n",
        "    img_weights, counts_recalc = compute_image_weights(\n",
        "        train_ds_full, LUT, NUM_CLASSES,\n",
        "        ignore_index=IGNORE_INDEX,\n",
        "        gamma=1.1, lambda_mix=0.9,\n",
        "        cap_min=0.25, cap_max=6.0, show_progress=True,\n",
        "    )\n",
        "    torch.save((img_weights, counts_recalc), \"img_weights_sampler_2.pt\")\n",
        "\n",
        "sampler = WeightedRandomSampler(\n",
        "    weights=img_weights,\n",
        "    num_samples=len(train_ds_full),\n",
        "    replacement=True\n",
        ")\n",
        "\n",
        "def safe_collate(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    xs = [x.contiguous().clone() for x in xs]  #avoid views interfering with dataloader pins\n",
        "    ys = [y.contiguous().clone() for y in ys]\n",
        "    return torch.stack(xs, 0), torch.stack(ys, 0)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds_full, batch_size=BATCH,\n",
        "    num_workers=12, persistent_workers=True,\n",
        "    pin_memory=True, prefetch_factor=4,\n",
        "    sampler=sampler, collate_fn=safe_collate\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_ds, batch_size=max(1, BATCH), shuffle=False,\n",
        "    num_workers=12, persistent_workers=True,\n",
        "    pin_memory=True, prefetch_factor=4, collate_fn=safe_collate\n",
        ")\n",
        "\n",
        "print(f\"Train batches: {len(train_loader)} | Val batches: {len(val_loader)}\")  #sanity check on iteration sizes\n"
      ],
      "metadata": {
        "id": "VXKY4A8auddi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==== model + optimizer ====\n",
        "#build deeplabv3+ with the chosen encoder, optionally warm-start from prior weights\n",
        "model = smp.DeepLabV3Plus(\n",
        "    encoder_name=enc,\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=NUM_CLASSES,\n",
        ").to(device)\n",
        "\n",
        "#warm start (optional)\n",
        "prev_path = \"bestprevdeeplabv3+.pth\"\n",
        "if os.path.exists(prev_path):\n",
        "    try:\n",
        "        state = torch.load(prev_path, map_location=device)\n",
        "        model.load_state_dict(state, strict=False)  #strict false so minor head/aux diffs don’t break load\n",
        "        print(f\"Loaded existing weights from {prev_path} (strict=False).\")\n",
        "    except Exception as e:\n",
        "        print(\"Warm start skipped:\", e)\n",
        "\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)  #adamw has stable weight decay\n",
        "\n",
        "#==== cosine schedule with 1-epoch warmup (per-step) ====\n",
        "#i compute per-step lr because grad accumulation changes effective steps per epoch\n",
        "steps_per_epoch = max(1, len(train_loader) // max(1, ACCUM_STEPS))\n",
        "total_steps     = EPOCHS * steps_per_epoch\n",
        "warmup_steps    = steps_per_epoch * 1\n",
        "\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        return float(step + 1) / float(warmup_steps)  #linear warmup\n",
        "    progress = (step - warmup_steps) / max(1, (total_steps - warmup_steps))\n",
        "    return 0.5 * (1.0 + math.cos(math.pi * progress))  #cosine decay\n",
        "\n",
        "sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "\n",
        "#==== losses: CE + Lovasz + per-class focal CE + OHEM (top-k CE) ====\n",
        "#final hybrid loss that balances overall accuracy, boundaries, hard pixels, and rare classes\n",
        "ce = torch.nn.CrossEntropyLoss(\n",
        "    weight=class_weights, ignore_index=IGNORE_INDEX, label_smoothing=0.02\n",
        ")\n",
        "lovasz = smp.losses.LovaszLoss(mode=\"multiclass\", per_image=False)\n",
        "\n",
        "#per-class alpha from CE weights, bump rare class (8) a bit more\n",
        "alpha_vec = (class_weights / class_weights.sum()).to(device).clone()\n",
        "alpha_vec[8] *= 1.50   #slightly emphasize road-blocked in my setup\n",
        "alpha_vec = alpha_vec / alpha_vec.sum()\n",
        "\n",
        "#per-class gammas: stronger on rare\n",
        "gamma_map = torch.full((NUM_CLASSES,), 1.5, device=device)\n",
        "gamma_map[8] = 2.4     #focus even more on specific difficult class\n",
        "\n",
        "def focal_ce_loss_per_class_gamma(logits, y, alpha, gamma_map, ignore_index=IGNORE_INDEX):\n",
        "    logp = F.log_softmax(logits, dim=1)\n",
        "    p    = torch.exp(logp)\n",
        "\n",
        "    B, C, H, W = logits.shape\n",
        "    y_flat     = y.view(B, -1)\n",
        "    logp_flat  = logp.view(B, C, -1)\n",
        "    p_flat     = p.view(B, C, -1)\n",
        "\n",
        "    valid = (y_flat != ignore_index)\n",
        "    idx   = y_flat.clone()\n",
        "    idx[~valid] = 0\n",
        "\n",
        "    pt    = torch.gather(p_flat,  1, idx.unsqueeze(1)).squeeze(1)   #prob of the true class per pixel\n",
        "    logpt = torch.gather(logp_flat,1, idx.unsqueeze(1)).squeeze(1)  #log-prob of the true class per pixel\n",
        "    alpha_per_pix = alpha[idx]                                       #class weighting by rarity\n",
        "    gamma_per_pix = gamma_map[idx]                                   #class-specific focusing\n",
        "\n",
        "    focal = -(alpha_per_pix * (1.0 - pt).pow(gamma_per_pix) * logpt)\n",
        "    focal = focal * valid\n",
        "    return focal.sum() / valid.sum().clamp_min(1)\n",
        "\n",
        "def topk_ce(logits, y, k=0.2):\n",
        "    \"\"\"\n",
        "    Online hard example mining for segmentation.\n",
        "    k: fraction of valid pixels to keep (e.g., 0.2 = top 20% hardest).\n",
        "    \"\"\"\n",
        "    per_pix = F.cross_entropy(logits, y, weight=class_weights,\n",
        "                              ignore_index=IGNORE_INDEX, reduction='none')\n",
        "    valid = (y != IGNORE_INDEX)\n",
        "    hard = per_pix[valid]\n",
        "    if hard.numel() == 0:\n",
        "        return torch.tensor(0.0, device=logits.device)\n",
        "    k_keep = max(1, int(k * hard.numel()))\n",
        "    vals, _ = torch.topk(hard, k_keep)\n",
        "    return vals.mean()\n",
        "\n",
        "def loss_fn(logits, y):\n",
        "    ce_term  = ce(logits, y)\n",
        "    lov_term = lovasz(logits, y)\n",
        "    foc_term = focal_ce_loss_per_class_gamma(\n",
        "        logits, y, alpha=alpha_vec, gamma_map=gamma_map, ignore_index=IGNORE_INDEX\n",
        "    )\n",
        "    ohem_term = topk_ce(logits, y, k=0.2)  #0.15..0.25 works; i use 0.2 here\n",
        "    #keep total weight ~1.0; this mix matched my validation behavior best\n",
        "    return 0.35 * ce_term + 0.30 * lov_term + 0.20 * foc_term + 0.15 * ohem_term\n",
        "\n",
        "#==== amp scaler: only for fp16, not bf16 ====\n",
        "scaler = torch.amp.GradScaler(enabled=SCALER_ENABLED)\n",
        "print(\"got losses / optimizer / AMP / scheduler\")  #checkpoint log\n"
      ],
      "metadata": {
        "id": "chc9AN1bujKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#==== ema wrapper (validate & save with ema weights) ====\n",
        "#ema keeps a smoothed copy of weights that usually validates more stably than raw steps\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {}\n",
        "        self.backup = {}\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[name] = p.data.clone()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.shadow[name].mul_(self.decay).add_(p.data, alpha=1.0 - self.decay)\n",
        "\n",
        "    def apply_shadow(self, model):\n",
        "        self.backup = {}\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad:\n",
        "                self.backup[name] = p.data.clone()\n",
        "                p.data.copy_(self.shadow[name])\n",
        "\n",
        "    def restore(self, model):\n",
        "        for name, p in model.named_parameters():\n",
        "            if p.requires_grad and name in self.backup:\n",
        "                p.data.copy_(self.backup[name])\n",
        "        self.backup = {}\n",
        "\n",
        "ema = EMA(model, decay=0.999)  #create the ema tracker\n",
        "\n",
        "#==== metrics ====\n",
        "#helper to average iou while ignoring background class\n",
        "def average_miou_ex_background(per_class_ious, bg_id=0):\n",
        "    arr = np.array(per_class_ious, dtype=np.float64)\n",
        "    if arr.size <= 1: return float(\"nan\")\n",
        "    classes = np.delete(arr, bg_id)\n",
        "    return float(np.nanmean(classes) * 100.0)\n",
        "\n",
        "@torch.no_grad()\n",
        "def miou_on_loader_fast(model, loader, n_classes, device,\n",
        "                        ignore_index=None, use_amp=True, show_progress=True, tta_hflip=True):\n",
        "    #fast evaluation with optional multi-scale and horizontal flip tta\n",
        "    def _forward_resized(m, imgs, scale):\n",
        "        H, W = imgs.shape[-2], imgs.shape[-1]\n",
        "        if scale != 1.0:\n",
        "            imgs = F.interpolate(imgs, scale_factor=scale, mode=\"bilinear\", align_corners=False)\n",
        "        pad_h = (32 - imgs.shape[-2] % 32) % 32\n",
        "        pad_w = (32 - imgs.shape[-1] % 32) % 32\n",
        "        imgs_p = F.pad(imgs, (0, pad_w, 0, pad_h)) if (pad_h or pad_w) else imgs\n",
        "        out = m(imgs_p)\n",
        "        out = out[..., :imgs.shape[-2], :imgs.shape[-1]]\n",
        "        return F.interpolate(out, size=(H, W), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "    model.eval()\n",
        "    cm = np.zeros((n_classes, n_classes), dtype=np.int64)  #confusion matrix\n",
        "\n",
        "    iterator = loader\n",
        "    pbar = None\n",
        "    if show_progress:\n",
        "        try:\n",
        "            iterator = tqdm(loader, desc=\"Evaluating\", total=len(loader), leave=True)\n",
        "            pbar = iterator\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    dev_type = device if isinstance(device, \"str\") else getattr(device, \"type\", \"cpu\")\n",
        "    autocast_dtype = (torch.bfloat16 if (use_amp and torch.cuda.is_available() and torch.cuda.is_bf16_supported())\n",
        "                      else (torch.float16 if use_amp else torch.float32))\n",
        "    scales = [0.85, 1.0, 1.15] if tta_hflip else [1.0]\n",
        "\n",
        "    try:\n",
        "        for images, labels in iterator:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            if use_amp:\n",
        "                with torch.autocast(device_type=dev_type, dtype=autocast_dtype):\n",
        "                    logits = 0\n",
        "                    for s in scales:\n",
        "                        l = _forward_resized(model, images, s)\n",
        "                        if tta_hflip:\n",
        "                            lf = _forward_resized(model, torch.flip(images, dims=[-1]), s)\n",
        "                            lf = torch.flip(lf, dims=[-1])\n",
        "                            l = 0.5 * (l + lf)\n",
        "                        logits = logits + l\n",
        "                    logits = logits / len(scales)\n",
        "            else:\n",
        "                logits = 0\n",
        "                for s in scales:\n",
        "                    l = _forward_resized(model, images, s)\n",
        "                    if tta_hflip:\n",
        "                        lf = _forward_resized(model, torch.flip(images, dims=[-1]), s)\n",
        "                        lf = torch.flip(lf, dims=[-1])\n",
        "                        l = 0.5 * (l + lf)\n",
        "                    logits = logits + l\n",
        "                logits = logits / len(scales)\n",
        "\n",
        "            preds = torch.argmax(logits, dim=1)  #class per pixel\n",
        "\n",
        "            yt = labels.reshape(-1)\n",
        "            yp = preds.reshape(-1)\n",
        "\n",
        "            if ignore_index is not None:\n",
        "                keep = yt != ignore_index\n",
        "                yt = yt[keep]; yp = yp[keep]\n",
        "\n",
        "            keep = (yt >= 0) & (yt < n_classes) & (yp >= 0) & (yp < n_classes)\n",
        "            yt = yt[keep].to(torch.int64); yp = yp[keep].to(torch.int64)\n",
        "            if yt.numel() == 0:\n",
        "                continue\n",
        "\n",
        "            idx = yt * n_classes + yp\n",
        "            counts = torch.bincount(idx, minlength=n_classes * n_classes)\n",
        "            cm += counts.cpu().numpy().reshape(n_classes, n_classes)\n",
        "\n",
        "            if pbar is not None:\n",
        "                tp_live = np.diag(cm).astype(np.float64)\n",
        "                fn_live = cm.sum(axis=1) - np.diag(cm)\n",
        "                fp_live = cm.sum(axis=0) - np.diag(cm)\n",
        "                denom_live = tp_live + fn_live + fp_live\n",
        "                with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "                    ious_live = np.where(denom_live > 0, tp_live / denom_live, np.nan)\n",
        "                pbar.set_postfix(mIoU=np.nanmean(ious_live))\n",
        "\n",
        "    finally:\n",
        "        if pbar is not None:\n",
        "            pbar.close()\n",
        "\n",
        "    #==== final metrics from confusion matrix ====\n",
        "    tp = np.diag(cm).astype(np.float64)\n",
        "    fn = cm.sum(axis=1) - np.diag(cm)\n",
        "    fp = cm.sum(axis=0) - np.diag(cm)\n",
        "    denom_iou = tp + fn + fp\n",
        "\n",
        "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "        ious = np.where(denom_iou > 0, tp / denom_iou, np.nan)\n",
        "\n",
        "    miou = float(np.nanmean(ious))\n",
        "\n",
        "    #per-class pixel accuracy: tp/(tp+fn)\n",
        "    denom_acc = tp + fn\n",
        "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
        "        per_class_acc = np.where(denom_acc > 0, tp / denom_acc, np.nan)\n",
        "\n",
        "    #overall pixel accuracy across all valid classes\n",
        "    total_pixels = cm.sum()\n",
        "    overall_acc = float(tp.sum() / total_pixels) if total_pixels > 0 else float(\"nan\")\n",
        "\n",
        "    return ious.tolist(), miou, per_class_acc.tolist(), overall_acc\n"
      ],
      "metadata": {
        "id": "G64fjBkBuvTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===== training loop =====\n",
        "#main training: amp + ema + cosine schedule + hybrid loss; saves best ema checkpoint\n",
        "best_miou, best_path = -1.0, \"best_deeplabv3p_ema.pth\"\n",
        "print(\"beginning the actual loop\")\n",
        "AMP_CAST_DTYPE = AMP_DTYPE or (torch.float16 if device.type == \"cuda\" else torch.float32)\n",
        "\n",
        "class EMAHook:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.ema = EMA(model, decay)\n",
        "\n",
        "ema_hook = EMAHook(model, decay=0.999)\n",
        "\n",
        "global_step = 0\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch:02d} [train]\", ncols=100, leave=False)\n",
        "    model.train()\n",
        "    running = 0.0\n",
        "    opt.zero_grad(set_to_none=True)\n",
        "\n",
        "    for step, (x, y) in enumerate(pbar, 1):\n",
        "        x = x.to(device, non_blocking=True)\n",
        "        y = y.to(device, dtype=torch.long, non_blocking=True)\n",
        "\n",
        "        #amp\n",
        "        with torch.amp.autocast(device_type=device.type, dtype=AMP_CAST_DTYPE, enabled=(device.type == \"cuda\")):\n",
        "            logits = model(x)\n",
        "            loss = loss_fn(logits, y)\n",
        "\n",
        "        if SCALER_ENABLED:\n",
        "            scaler.scale(loss/ACCUM_STEPS).backward()  #scale for fp16 stability\n",
        "        else:\n",
        "            (loss/ACCUM_STEPS).backward()\n",
        "\n",
        "        need_step = (step % ACCUM_STEPS == 0) or (step == len(train_loader))\n",
        "        if need_step:\n",
        "            #grad clip\n",
        "            if SCALER_ENABLED:\n",
        "                scaler.unscale_(opt)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=GRAD_CLIP_N)\n",
        "\n",
        "            if SCALER_ENABLED:\n",
        "                scaler.step(opt); scaler.update()\n",
        "            else:\n",
        "                opt.step()\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            ema_hook.ema.update(model)  #update ema shadow after optimizer step\n",
        "            sched.step()\n",
        "            global_step += 1\n",
        "\n",
        "        running += loss.item() * x.size(0)\n",
        "        pbar.set_postfix(loss=f\"{loss.item():.3f}\", lr=f\"{sched.get_last_lr()[0]:.2e}\")  #live feedback\n",
        "\n",
        "    train_loss = running / len(train_loader.dataset)\n",
        "\n",
        "    #==== validate with ema weights ====\n",
        "    ema_hook.ema.apply_shadow(model)\n",
        "    per_class_ious, val_miou, per_class_acc, overall_acc = miou_on_loader_fast(\n",
        "        model, val_loader, NUM_CLASSES, device, ignore_index=IGNORE_INDEX, tta_hflip=True\n",
        "    )\n",
        "    avg_miou_pct = float(np.nanmean(np.array(per_class_ious[1:], dtype=np.float64)) * 100.0)  #exclude background\n",
        "    ema_hook.ema.restore(model)\n",
        "\n",
        "    #nice prints\n",
        "    print(\"Per-class IoUs:\", per_class_ious)\n",
        "    print(\"Per-class Accuracies:\", [None if np.isnan(a) else float(a) for a in per_class_acc])\n",
        "    print(f\"Overall Pixel Acc: {overall_acc*100:.2f}%\")\n",
        "    print(f\"Avg mIoU (no background): {avg_miou_pct:.2f}%\")\n",
        "    print(f\"Epoch {epoch:02d} | train_loss {train_loss:.4f} | val_mIoU (EMA+TTA) {val_miou:.3f}\")\n",
        "\n",
        "    if val_miou > best_miou:\n",
        "        best_miou = val_miou\n",
        "        #save ema weights as best\n",
        "        ema_hook.ema.apply_shadow(model)\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        ema_hook.ema.restore(model)\n",
        "        print(f\"  ↳ saved best EMA weights to {best_path}\")\n",
        "        version = 13\n",
        "        torch.save(model.state_dict(), f\"{BASE}/v{version}_deeplabv3p_ema_e{epoch}miou{val_miou:.3f}.pth\")\n",
        "        print(\"saved to drive\")\n",
        "\n",
        "#gpu vram info\n",
        "if torch.cuda.is_available():\n",
        "    free, total = torch.cuda.mem_get_info()\n",
        "    print(f\"GPU VRAM used: {(total-free)/1e9:.2f} GB / {total/1e9:.2f} GB  ({torch.cuda.get_device_name(0)})\")\n",
        "\n",
        "print(f\"Done. Best EMA+TTA val mIoU: {best_miou:.3f}\")  #final summary so i can compare runs\n"
      ],
      "metadata": {
        "id": "txu00trxuwdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S37nXli4_CnZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}